{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Project - WeRateDogs Dataset\n",
    "\n",
    "### Table of content\n",
    "- [Project Detail](#detail)\n",
    "- [Gathering Data](#gather)\n",
    "- [Assessing Data](#assess)\n",
    "    - [Data issues](#issues)\n",
    "- [Cleaning Data](#clean)\n",
    "    - [Cleaning summary](#summary)\n",
    "- [Storing Data](#store)\n",
    "- [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Detail <a class=\"anchor\" id=\"detail\"></a>\n",
    "\n",
    "This Project consist in analyze three different types of archives and Wrangle this data. To complete this process we will gather, assess, clean and store the new data for visualization and analysis. The dataset for this task is from Twitter account WeRateDogs that rates people's dog with a lot of humor, showing funny pictures and comments and a unique rating level. The purpose of this task is to put in practice what I learned in the lesson of Data Wrangling from Udacity Data Analyst Nanodegree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data <a class=\"anchor\" id=\"gather\"></a>\n",
    "In this step, data will be collected from three sources and dataframes will be made. These data will be obtained as following:\n",
    "1. **Twitter archive file**: the *twitter-archive-enhanced.csv* was provided by Udacity in Data Wrangling project website ([**link for download**](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv)). The ***df_tweets*** will be made with pandas *read_csv* command.\n",
    "2. **Tweet image predictions**: the *image-predictions.tsv* will be downloaded programmatically using requests library and the url where Udacity hosts the file (*https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv*). This file contains breeds classification based on the AI predictions of the dog images. The ***df_predictions*** will be made with pandas *read_csv* command and the argument *sep='\\t'* as the file has the tsv extension.\n",
    "3. **Twitter API JSON**: the *tweet_json.txt* will be made by querying the Twitter API for each tweet ID of the first file using PythonÂ´s Tweepy library and storing each tweet JSON data in this text file. With Python's *json* library, the text file will be read line by line and a pandas dataframe(***df_tweets_api***) will be made with tweet ID, retweet count, favourite count and retweeted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data <a class=\"anchor\" id=\"assess\"></a>\n",
    "Once the three dataframes were obtained, I will assess them visually and programmatically to check the data. For the former, I will use the Jupiter Notebook to print the three dataframes and for that I changed the visualization parameters with the command *pd.set_option()* to show the entire dataframe. For the later, I will use pandas methods as *info*, *duplicated*, *unique*, *value_counts* and etc. \n",
    "\n",
    "Then the problems encountered will be cataloged as quality issues and tidiness issues.\n",
    "1. Quality: issues with content. Low quality data is also known as dirty data;\n",
    "2. Tidiness: issues with structure that prevent easy analysis. Untidy data is also known as messy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data issues <a class=\"anchor\" id=\"issues\"></a>\n",
    "\n",
    "#### Quality\n",
    "1. The url is repeated in some cases in column expanded_urls of the df_tweets table;\n",
    "2. Columns with id as float value in df_tweets table;\n",
    "3. Rating numerator in df_tweets int instead of float, numbers with decimal values;\n",
    "4. Rating numerator in df_tweets with wrong values, numbers with decimal values;\n",
    "5. Wrong classification for IDs 835246439529840640 and 666287406224695296 in df_tweets table;\n",
    "6. Keep original ratings of df_tweets table;\n",
    "7. 66 duplicated jpg_url in df_predictions table;\n",
    "8. Multiples predictions but only need the first correct one found in df_predictions table;\n",
    "9. Some columns won't be used for analysis.\n",
    "\n",
    "#### Tidiness\n",
    "10. Stage of dog in multiples columns(doggo, floofer, pupper and puppo columns);\n",
    "11. All tables should be only one dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data <a class=\"anchor\" id=\"clean\"></a>\n",
    "In this step the issues found in Assessing Data will be corrected following the process of defining the problem, coding the correction and testing the code. First, to avoid any problem of data loss in the correction process, we will create copies of all dataframes respecting their names without the df at the beginning and with the word clean at the end. Whenever a mistake has been made, the copy can be restarted without losing the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning summary <a class=\"anchor\" id=\"summary\"></a>\n",
    "\n",
    "#### Quality\n",
    "1. Split the url using comma and use only url from twitter;\n",
    "2. Change type of ID's columns to integer;\n",
    "3. Change type of rating_numerator column to float;\n",
    "4. Correct the decimal values of rating_numerator column;\n",
    "5. Correct the classification of IDs 835246439529840640 and 666287406224695296;\n",
    "6. Delete rows with retweeted_status_user_id;\n",
    "7. Drop duplicated values of jpg_url;\n",
    "8. Create a prediction and a confidence columns in the dataframe with the first true prediction found, searching from p1 to p3.;\n",
    "9. Drop columns not used for analysis.\n",
    "\n",
    "#### Tidiness\n",
    "10. Group all the columns that represent stage of dog;\n",
    "11. Merge all dataframes into one. Inner is used to make a complete dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data <a class=\"anchor\" id=\"store\"></a>\n",
    "Use Pandas *to_csv* method to create a new CSV file (*twitter_archive_master.csv*) to store the result of the Data Wranglind process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a class=\"anchor\" id=\"conclusion\"></a>\n",
    "Data Wrangling is a core skill for Data Analyst. Mastering the python language and the tolls used in this project are well recommended to work with a huge amount and different data sources. It's unbelievable to think about doing all this work manually and the technology is here to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
